{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2023-06-02T14:30:13.499816Z","iopub.execute_input":"2023-06-02T14:30:13.500099Z","iopub.status.idle":"2023-06-02T14:30:28.667241Z","shell.execute_reply.started":"2023-06-02T14:30:13.500056Z","shell.execute_reply":"2023-06-02T14:30:28.666040Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.29.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.14.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (5.4.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.5.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.28.2)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom transformers import ElectraTokenizer, ElectraForSequenceClassification, AdamW\nfrom torch.utils.data import DataLoader, Dataset\nimport torch\nfrom tqdm import tqdm\n\nclass CustomTrainset(Dataset):\n    def __init__(self, data, tokenizer, max_length):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        content = str(self.data.iloc[index][\"content\"])\n        label = self.data.iloc[index][\"label\"]\n        encoding = self.tokenizer.encode_plus(\n            content,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors=\"pt\",\n        )\n        return {\n            \"input_ids\": encoding[\"input_ids\"].flatten(),\n            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n            \"label\": torch.tensor(label, dtype=torch.long),\n\n        }\n\nclass CustomTestset(Dataset):\n    def __init__(self, data, tokenizer, max_length):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        content = str(self.data.iloc[index][\"content\"])\n        encoding = self.tokenizer.encode_plus(\n            content,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors=\"pt\",\n        )\n        return {\n            \"input_ids\": encoding[\"input_ids\"].flatten(),\n            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n        }\n\n    \ndef train_model(model, train_loader, optimizer, device):\n    model.train()\n    model.to(device)\n\n    for batch in train_loader:\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"label\"].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n\ndef evaluate_model(model, val_loader, device):\n    model.eval()\n    model.to(device)\n\n    val_loss = 0\n    correct_predictions = 0\n\n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"label\"].to(device)\n\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            logits = outputs.logits\n\n            val_loss += loss.item()\n\n            preds = torch.argmax(logits, dim=1)\n            correct_predictions += torch.sum(preds == labels).item()\n\n    val_loss /= len(val_loader)\n    accuracy = correct_predictions / len(val_loader.dataset)\n\n    return val_loss, accuracy\n\n\n# 데이터 로드\ntrain_data = pd.read_csv(\"/kaggle/input/jbnu-sw-ai/train_f.csv\")\ntrain_data = train_data[[\"content\", \"label\"]]\n\ntest_data = pd.read_csv(\"/kaggle/input/jbnu-sw-ai/test_f.csv\")\ntest_data = test_data[[\"content\"]]\n\nle = preprocessing.LabelEncoder()\ntrain_data[\"label\"] = le.fit_transform(train_data[\"label\"])\n\n# 데이터 전처리\ntokenizer = ElectraTokenizer.from_pretrained(\"google/electra-base-discriminator\")\nmax_length = 512\ntrain_data, val_data = train_test_split(train_data, test_size=0.3, random_state=42)\n\ntrain_dataset = CustomTrainset(train_data, tokenizer, max_length)\nval_dataset = CustomTrainset(val_data, tokenizer, max_length)\ntest_dataset = CustomTestset(test_data, tokenizer, max_length)\n\n# 데이터 로더 생성\nbatch_size = 18\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-02T14:30:28.670118Z","iopub.execute_input":"2023-06-02T14:30:28.670497Z","iopub.status.idle":"2023-06-02T14:30:54.594184Z","shell.execute_reply.started":"2023-06-02T14:30:28.670460Z","shell.execute_reply":"2023-06-02T14:30:54.593344Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19936dfaccac432c8adf4c1df489e45a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/27.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b1b300873dd4695bc4a02f9bf6a09de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f34ef0d28464525a0f68b4532d90916"}},"metadata":{}}]},{"cell_type":"code","source":"# 디바이스 설정\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ELECTRA 모델 초기화\nnum_labels = 24\nmodel = ElectraForSequenceClassification.from_pretrained(\"google/electra-base-discriminator\", num_labels=num_labels)\n\n# 옵티마이저 설정\noptimizer = AdamW(model.parameters(), lr=2e-5)\n\n# 모델 학습\nepochs = 5\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch + 1}/{epochs}\")\n    train_model(model, train_loader, optimizer, device)\n    val_loss, val_accuracy = evaluate_model(model, val_loader, device)\n    print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n\n# 모델 저장\ntorch.save(model.state_dict(), \"/kaggle/working/electra_base_model.pth\")","metadata":{"execution":{"iopub.status.busy":"2023-06-02T14:30:54.595492Z","iopub.execute_input":"2023-06-02T14:30:54.595926Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"056654710f294b3b83eeea05ab1bed20"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5\n","output_type":"stream"}]},{"cell_type":"code","source":"# 테스트 데이터로 예측\npredictions = []\nmodel.eval()\nmodel.to(device)\n\nwith torch.no_grad():\n    for batch in tqdm(test_loader):\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n\n        outputs = model(input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n\n        preds = torch.argmax(logits, dim=1)\n        predictions.extend(preds.cpu().numpy())\n\npreds = le.inverse_transform(predictions)\n\nsubmit = pd.read_csv(\"/kaggle/input/jbnu-sw-ai/sample_submission.csv\")\nsubmit[\"label\"] = preds\nsubmit.to_csv(\"/kaggle/working/electra-base-prompt.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}